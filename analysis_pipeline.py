
import gc
import re
import logging
import numpy as np
from datetime import datetime
from enum import Enum, auto
from typing import Dict, List, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
from transformers import pipeline
import torch

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('pipeline_analysis.log')
    ]
)
logger = logging.getLogger(__name__)


class ModelType(Enum):
    FACT_CHECK = auto()
    SUMMARIZER = auto()
    SENTIMENT = auto()


@dataclass
class AnalysisResult:
    article_id: str
    title: str
    summary: str = ""
    sentiment: Dict[str, Union[str, float]] = None
    credibility: Dict[str, float] = None
    embedding: List[float] = None
    processing_time: float = 0.0
    success: bool = False
    error: str = ""
    model_versions: Dict[str, str] = None
    metadata: Dict[str, Any] = None

    def get(self, key: str, default=None):
        return getattr(self, key, default)

    def to_dict(self) -> Dict:
        return {
            "article_id": self.article_id,
            "title": self.title,
            "summary": self.summary,
            "sentiment": self.sentiment,
            "credibility": self.credibility,
            "embedding": self.embedding,
            "processing_time": self.processing_time,
            "success": self.success,
            "error": self.error,
            "model_versions": self.model_versions,
            "metadata": self.metadata
        }


class NewsAnalysisPipeline:
    def __init__(self, max_workers=4, device=None):
        self.max_workers = max_workers
        self.device = device if device else "cuda" if torch.cuda.is_available() else "cpu"
        self.available_models = set()
        self.model_versions = {}
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        self._initialize_models()

    def _initialize_models(self):
        model_config = {
            ModelType.FACT_CHECK: {
                'model': 'typeform/distilbert-base-uncased-mnli',
                'task': 'text-classification'
            },
            ModelType.SUMMARIZER: {
                'model': 'csebuetnlp/mT5_multilingual_XLSum',
                'task': 'summarization'
            },
            ModelType.SENTIMENT: {
                'model': 'uer/roberta-base-finetuned-jd-binary-chinese',
                'task': 'text-classification'
            }
        }

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._load_hf_model, m_type, cfg['task'], cfg['model']): m_type
                for m_type, cfg in model_config.items()
            }

            with tqdm(total=len(futures), desc="åŠ è½½æ¨¡å‹") as pbar:
                for future in as_completed(futures):
                    m_type = futures[future]
                    try:
                        model_name = future.result()
                        self.model_versions[m_type] = model_name
                        self.available_models.add(m_type)
                        logger.info(f"{m_type.name}æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                    except Exception as e:
                        logger.error(f"{m_type.name}æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                    finally:
                        pbar.update(1)
                        gc.collect()

    def _load_hf_model(self, m_type: ModelType, task: str, model_name: str):
        pipe = pipeline(
            task,
            model=model_name,
            tokenizer=model_name,
            device=0 if self.device == "cuda" else -1
        )
        setattr(self, f"{m_type.name.lower()}_pipeline", pipe)
        return model_name

    def analyze_article(self, article: Dict[str, Any]) -> AnalysisResult:
        start_time = datetime.now()
        result = AnalysisResult(
            article_id=str(article.get("id", "")),
            title=article.get("metadata", {}).get("title", ""),
            model_versions={k.name: v for k, v in self.model_versions.items()},
            metadata=article.get("metadata", {})
        )

        try:
            raw_content = article.get("text", "")
            if not raw_content:
                raise ValueError("æ–‡ç« å†…å®¹ä¸ºç©º")

            cleaned_content = self._clean_baidu_content(raw_content)
            if len(cleaned_content) < 50:
                raise ValueError(f"æœ‰æ•ˆå†…å®¹è¿‡çŸ­({len(cleaned_content)}å­—ç¬¦)")

            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {
                    executor.submit(self._generate_summary, cleaned_content): "summary",
                    executor.submit(self._check_credibility, cleaned_content): "credibility",
                    executor.submit(self._analyze_sentiment, cleaned_content): "sentiment"
                }

                for future in as_completed(futures):
                    task_type = futures[future]
                    try:
                        setattr(result, task_type, future.result())
                    except Exception as e:
                        logger.error(f"{task_type}åˆ†æå¤±è´¥: {str(e)}")
                        if task_type == "credibility":
                            result.credibility = {"contradiction": 0.0, "neutral": 1.0, "entailment": 0.0}
                        elif task_type == "sentiment":
                            result.sentiment = {"label": "neutral", "score": 0.0, "truncated_length": 0}

            result.success = True
        except Exception as e:
            result.error = str(e)
            result.success = False

        result.processing_time = (datetime.now() - start_time).total_seconds()
        return result

    def _clean_baidu_content(self, content: str) -> str:
        error_patterns = [
            r"We're sorry but",
            r"ç™¾åº¦APPæ‰«ç æŸ¥çœ‹",
            r"è¯·åœ¨æ–‡ä¸­æ¨ªçº¿å¤„è¡¥å†™æ°å½“çš„è¯­å¥",
            r"æœ¬é¢˜è¯•å·æ¥æº",
            r"æœç´¢è¯•é¢˜"
        ]

        if any(re.search(pattern, content) for pattern in error_patterns):
            return ""

        cleaned = re.sub(r'<[^>]+>', '', content)
        cleaned = re.sub(r'[\x00-\x1F\x7F]', '', cleaned)
        cleaned = re.sub(r'\[\d+\]|\uff3b\d+\uff3d', '', cleaned)
        cleaned = re.sub(r'å›¾ç‰‡:.*?\]', '', cleaned)
        cleaned = re.sub(r'å›¾ç‰‡æ¥æº:.*', '', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        # ç§»é™¤æ‰€æœ‰è§†é¢‘ç›¸å…³æè¿°
        content = re.sub(r'è§†é¢‘å¤„ç†ä¸­.*?æ›´æ¢å°é¢', '', content)
        content = re.sub(r'ç®€ä»‹:.*?æ¥æº:', '', content)
        content = re.sub(r'î˜§ \d+:\d+', '', content)  # ç§»é™¤æ—¶é—´æ ‡è®°

        # ç§»é™¤æ¥æºä¿¡æ¯
        content = re.sub(r'æ¥æº:.*?(å‘å¸ƒæ—¶é—´:)?.*?\n', '\n', content)


        if len(cleaned) < 50:
            return ""

        sentences = re.split(r'[\u3002ï¼ï¼Ÿ!?.]', cleaned)
        unique_sentences = set(sentences)
        if len(unique_sentences) / len(sentences) < 0.5:
            return ""
        # æ·»åŠ è°ƒè¯•è¯­å¥
        print("æ¸…æ´—åå†…å®¹ï¼š", cleaned[:500])
        return cleaned

    def _generate_summary(self, text: str) -> str:
        if ModelType.SUMMARIZER not in self.available_models:
            return text[:150] + "..."

        try:
            chunks = [text[i:i + 1024] for i in range(0, len(text), 1024)]
            summaries = [
                self.summarizer_pipeline(
                    chunk,
                    max_length=256,
                    min_length=128,
                    num_beams=6,
                    length_penalty=1.5,
                    no_repeat_ngram_size=3,
                    truncation=True,
                    do_sample=False
                )[0]['summary_text'] for chunk in chunks
            ]
            return "".join(summaries)[:256]
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return text[:150] + "..."

    def _check_credibility(self, text: str) -> Dict[str, float]:
        """
        ä½¿ç”¨äº‹å®æ ¸æŸ¥æ¨¡å‹åˆ†ææ–‡æœ¬å¯ä¿¡åº¦
        è¿”å›æ ¼å¼: {"contradiction": 0.0, "neutral": 1.0, "entailment": 0.0}
        """
        if ModelType.FACT_CHECK not in self.available_models:
            logger.warning("äº‹å®æ ¸æŸ¥æ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤ä¸­æ€§è¯„åˆ†")
            return {"contradiction": 0.0, "neutral": 1.0, "entailment": 0.0}

        try:
            # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
            if not isinstance(text, str) or not text.strip():
                raise ValueError("è¾“å…¥æ–‡æœ¬æ— æ•ˆ")

            # åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬ï¼ˆæ¯æ®µ512å­—ç¬¦ï¼‰
            segments = [text[i:i + 512] for i in range(0, min(len(text), 2048), 512)]
            if not segments:
                return {"contradiction": 0.0, "neutral": 1.0, "entailment": 0.0}

            total_scores = {"contradiction": 0.0, "neutral": 0.0, "entailment": 0.0}
            valid_segments = 0

            for segment in segments:
                try:
                    # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
                    if not segment.strip():
                        continue

                    # è·å–æ¨¡å‹ç»“æœ
                    result = self.fact_check_pipeline(
                        segment,
                        truncation=True,
                        max_length=512
                    )

                    # å¤„ç†ä¸åŒè¿”å›æ ¼å¼
                    if isinstance(result, list) and len(result) > 0:
                        item = result[0]
                        if isinstance(item, dict):
                            # æ–°ç‰ˆtransformersè¿”å›æ ¼å¼
                            label = item["label"].lower()
                            score = float(item["score"])
                        elif isinstance(item, list):
                            # æ—§ç‰ˆå¯èƒ½è¿”å›æ ¼å¼
                            label = item[0]["label"].lower()
                            score = float(item[0]["score"])
                        else:
                            continue

                        # ç´¯åŠ åˆ†æ•°
                        if label in total_scores:
                            total_scores[label] += score
                            valid_segments += 1

                except Exception as seg_error:
                    logger.warning(f"åˆ†æ®µåˆ†æå¤±è´¥: {str(seg_error)}", exc_info=True)
                    continue

            # è®¡ç®—å¹³å‡åˆ†
            if valid_segments > 0:
                return {k: round(v / valid_segments, 4) for k, v in total_scores.items()}

            return {"contradiction": 0.0, "neutral": 1.0, "entailment": 0.0}

        except Exception as e:
            logger.error(f"äº‹å®æ ¸æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
            return {
                "contradiction": 0.0,
                "neutral": 1.0,
                "entailment": 0.0,
                "error": str(e)
            }



    def _analyze_sentiment(self, text: str) -> Dict[str, Union[str, float]]:
        if ModelType.SENTIMENT not in self.available_models:
            return {"label": "neutral", "score": 0.0, "truncated_length": 0}

        try:
            sentiment_pipeline = getattr(self, 'sentiment_pipeline', None)
            if sentiment_pipeline is None:
                raise ValueError("æƒ…æ„Ÿæ¨¡å‹æœªåŠ è½½")

            max_length = 512
            truncated_text = text[:max_length]
            result = sentiment_pipeline(truncated_text)[0]

            label_map = {
                "LABEL_0": "negative",
                "LABEL_1": "positive",
                "LABEL_2": "neutral"
            }

            mapped_label = label_map.get(result['label'], result['label'])

            return {
                "label": mapped_label,
                "score": float(result['score']),
                "sentiment": self._get_sentiment_emoji(mapped_label, result['score']),
                "truncated_length": len(truncated_text)
            }
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}")
            return {"label": "error", "score": 0.0, "truncated_length": 0}

    def _get_sentiment_emoji(self, label: str, score: float) -> str:
        if "positive" in label:
            return "ğŸ˜Š" if score > 0.8 else "ğŸ™‚"
        elif "negative" in label:
            return "ğŸ˜ " if score > 0.8 else "ğŸ˜"
        return "ğŸ˜"
